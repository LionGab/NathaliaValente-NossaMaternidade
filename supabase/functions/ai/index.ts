/**
 * Nossa Maternidade - AI Edge Function (Production-Ready)
 *
 * NathIA: Parceira de bolso da m√£e brasileira
 *
 * ARQUITETURA DE PROVIDERS (ordem de prioridade):
 * 1. Gemini 2.5 Flash (DEFAULT) - R√°pido, direto, persona est√°vel
 * 2. Claude Sonnet 4.5 (FALLBACK) - Quando Gemini falha
 * 3. OpenAI GPT-4o (√öLTIMO RECURSO) - Emerg√™ncia
 *
 * CASOS ESPECIAIS:
 * - Imagens/Ultrassons ‚Üí Claude Vision (√∫nico default)
 * - Perguntas m√©dicas ‚Üí Gemini + Google Search (grounding)
 *
 * Features:
 * - JWT validation (authenticated users only)
 * - Rate limiting via Upstash Redis (20 req/min por usu√°rio)
 * - Circuit breakers (protege contra cascade failures)
 * - Structured logging & monitoring
 * - Payload caps (prevent abuse)
 * - Fallback chain: Gemini ‚Üí Claude ‚Üí OpenAI
 * - Grounding com Google Search (Gemini)
 * - Suporte a imagens (Claude Vision)
 * - Citations extra√≠das corretamente
 * - CORS restrito
 *
 * @version 2.1.0 - Circuit breakers implementados (2025-12)
 */

import { createClient } from "https://esm.sh/@supabase/supabase-js@2";
import { Anthropic } from "https://esm.sh/@anthropic-ai/sdk@0.28.0";
import OpenAI from "https://esm.sh/openai@4.89.0";
import { Redis } from "https://esm.sh/@upstash/redis@1.28.0";
import { CircuitBreaker } from "../_shared/circuit-breaker.ts";

// =======================
// STRUCTURED LOGGING
// =======================

type LogLevel = "info" | "warn" | "error" | "debug";

interface LogEntry {
  timestamp: string;
  level: LogLevel;
  event: string;
  requestId?: string;
  userId?: string; // Hashed for privacy
  data?: Record<string, unknown>;
  error?: {
    message: string;
    stack?: string;
    code?: string;
  };
}

interface RequestMetrics {
  requestId: string;
  userId: string;
  provider: string;
  model?: string;
  messageCount: number;
  estimatedInputTokens: number;
  actualInputTokens?: number;
  outputTokens?: number;
  totalTokens?: number;
  latencyMs: number;
  success: boolean;
  fallback: boolean;
  rateLimitSource: "redis" | "memory";
  hasImage: boolean;
  hasGrounding: boolean;
}

// =======================
// CRISIS & SAFETY DETECTION (NathIA v2.0)
// =======================

/**
 * Palavras-chave de CRISE - for√ßa uso de Claude (modelo mais seguro)
 * Se qualquer uma for detectada, N√ÉO usa Gemini
 */
const CRISIS_KEYWORDS = [
  // Idea√ß√£o suicida
  "suic√≠dio",
  "suicidio",
  "me matar",
  "quero morrer",
  "n√£o quero viver",
  "melhor morta",
  "vou me matar",
  "penso em morrer",
  "acabar com tudo",
  "n√£o aguento mais viver",
  "queria estar morta",
  // Risco ao beb√™
  "machucar o beb√™",
  "machucar meu filho",
  "machucar minha filha",
  "fazer mal ao beb√™",
  "jogar o beb√™",
  "sufocar o beb√™",
  // Automutila√ß√£o
  "me cortar",
  "me machucar",
  "me ferir",
  // Desespero extremo
  "n√£o tenho sa√≠da",
  "ningu√©m se importa",
  "sou um peso",
];

/**
 * Frases que NathIA NUNCA deve dizer
 * Se Gemini retornar qualquer uma, reprocessa com Claude
 */
const BLOCKED_PHRASES = [
  // Diagn√≥sticos proibidos
  "voc√™ tem depress√£o",
  "voc√™ tem ansiedade",
  "voc√™ est√° com depress√£o",
  "voc√™ est√° com ansiedade",
  "voc√™ sofre de",
  // Prescri√ß√µes proibidas
  "voc√™ precisa",
  "voc√™ deve",
  "voc√™ tem que",
  "√© obrigat√≥rio",
  // Depend√™ncia emocional
  "eu fico aqui",
  "pode contar comigo sempre",
  "estarei sempre aqui",
  "nunca vou te abandonar",
  // Culpa/press√£o
  "seu beb√™ precisa de voc√™",
  "pense no seu filho",
  "voc√™ √© ego√≠sta",
  "n√£o pode fazer isso",
];

/**
 * Detecta se mensagem indica crise (requer Claude)
 */
function isCrisis(message: string): boolean {
  const lower = message.toLowerCase();
  return CRISIS_KEYWORDS.some((k) => lower.includes(k));
}

/**
 * Detecta se resposta cont√©m frase bloqueada (requer reprocessamento)
 */
function hasBlockedPhrase(response: string): boolean {
  const lower = response.toLowerCase();
  return BLOCKED_PHRASES.some((p) => lower.includes(p));
}

// =======================
// MESSAGE TYPES
// =======================

interface AIMessage {
  role: "user" | "assistant" | "system";
  content: string;
}

interface ApiResponse {
  content: string;
  usage: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  provider: string;
  grounding?: boolean;
  citations?: string[];
}

interface ImageData {
  base64: string;
  mediaType: string;
}

/**
 * Hash userId for privacy in logs
 * Uses simple hash - sufficient for log anonymization
 */
function hashUserId(userId: string): string {
  let hash = 0;
  for (let i = 0; i < userId.length; i++) {
    const char = userId.charCodeAt(i);
    hash = (hash << 5) - hash + char;
    hash = hash & hash; // Convert to 32bit integer
  }
  return `user_${Math.abs(hash).toString(16).substring(0, 8)}`;
}

/**
 * Generate unique request ID
 */
function generateRequestId(): string {
  return `req_${Date.now().toString(36)}_${Math.random().toString(36).substring(2, 8)}`;
}

/**
 * Structured logger - outputs JSON for Supabase Logs / external ingestion
 */
const logger = {
  _log(level: LogLevel, event: string, data?: Record<string, unknown>, error?: Error) {
    const entry: LogEntry = {
      timestamp: new Date().toISOString(),
      level,
      event,
      ...(data && { data }),
      ...(error && {
        error: {
          message: error.message,
          stack: error.stack,
          code: (error as NodeJS.ErrnoException).code,
        },
      }),
    };

    // Output as JSON for structured logging
    const output = JSON.stringify(entry);

    switch (level) {
      case "error":
        console.error(output);
        break;
      case "warn":
        console.warn(output);
        break;
      case "debug":
        console.debug(output);
        break;
      default:
        console.log(output);
    }
  },

  info(event: string, data?: Record<string, unknown>) {
    this._log("info", event, data);
  },

  warn(event: string, data?: Record<string, unknown>) {
    this._log("warn", event, data);
  },

  error(event: string, error: Error, data?: Record<string, unknown>) {
    this._log("error", event, data, error);
  },

  debug(event: string, data?: Record<string, unknown>) {
    this._log("debug", event, data);
  },

  /**
   * Log request metrics (called at end of each request)
   */
  metrics(metrics: RequestMetrics) {
    this._log("info", "request_metrics", {
      requestId: metrics.requestId,
      userId: hashUserId(metrics.userId),
      provider: metrics.provider,
      model: metrics.model,
      messageCount: metrics.messageCount,
      tokens: {
        estimatedInput: metrics.estimatedInputTokens,
        actualInput: metrics.actualInputTokens,
        output: metrics.outputTokens,
        total: metrics.totalTokens,
      },
      latencyMs: metrics.latencyMs,
      success: metrics.success,
      fallback: metrics.fallback,
      rateLimitSource: metrics.rateLimitSource,
      features: {
        hasImage: metrics.hasImage,
        hasGrounding: metrics.hasGrounding,
      },
    });
  },

  /**
   * Log rate limit event
   */
  rateLimit(
    userId: string,
    type: "requests" | "tokens",
    current: number,
    max: number,
    source: "redis" | "memory"
  ) {
    this._log("warn", "rate_limit_exceeded", {
      userId: hashUserId(userId),
      type,
      current,
      max,
      source,
    });
  },

  /**
   * Log provider fallback
   */
  fallback(requestId: string, fromProvider: string, toProvider: string, reason: string) {
    this._log("warn", "provider_fallback", {
      requestId,
      fromProvider,
      toProvider,
      reason,
    });
  },

  /**
   * Log authentication event
   */
  auth(event: "success" | "failure", userId?: string, reason?: string) {
    this._log(event === "success" ? "info" : "warn", `auth_${event}`, {
      ...(userId && { userId: hashUserId(userId) }),
      ...(reason && { reason }),
    });
  },
};

// =======================
// ENV & CLIENTS
// =======================

const SUPABASE_URL = Deno.env.get("SUPABASE_URL")!;
const SUPABASE_SERVICE_KEY = Deno.env.get("SUPABASE_SERVICE_ROLE_KEY")!;
const ANTHROPIC_KEY = Deno.env.get("ANTHROPIC_API_KEY")!;
const GEMINI_KEY = Deno.env.get("GEMINI_API_KEY")!;
const OPENAI_KEY = Deno.env.get("OPENAI_API_KEY")!;

// Upstash Redis (opcional - fallback para in-memory se n√£o configurado)
const UPSTASH_REDIS_URL = Deno.env.get("UPSTASH_REDIS_REST_URL");
const UPSTASH_REDIS_TOKEN = Deno.env.get("UPSTASH_REDIS_REST_TOKEN");

// Dom√≠nios permitidos (CORS)
const ALLOWED_ORIGINS = [
  "https://nossamaternidade.com.br",
  "https://www.nossamaternidade.com.br",
  "exp://", // Expo Go
  "http://localhost:8081", // Dev local
];

const anthropic = new Anthropic({ apiKey: ANTHROPIC_KEY });
const openai = new OpenAI({ apiKey: OPENAI_KEY });

// Initialize Redis client (if configured)
let redis: Redis | null = null;
if (UPSTASH_REDIS_URL && UPSTASH_REDIS_TOKEN) {
  try {
    redis = new Redis({
      url: UPSTASH_REDIS_URL,
      token: UPSTASH_REDIS_TOKEN,
    });
    console.log("‚úÖ Upstash Redis initialized");
  } catch (err) {
    console.error("‚ö†Ô∏è Failed to initialize Redis, using in-memory fallback:", err);
  }
}

// =======================
// CIRCUIT BREAKERS
// =======================

/**
 * Circuit breakers para cada provider de IA
 * Evitam cascade failures quando um provider est√° inst√°vel
 */
const geminiCircuit = new CircuitBreaker(
  "gemini",
  {
    failureThreshold: 5, // 5 falhas consecutivas ‚Üí OPEN
    timeoutMs: 30_000, // 30s em OPEN antes de tentar HALF_OPEN
    halfOpenMaxCalls: 3, // 3 tentativas em HALF_OPEN
  },
  logger
);

const claudeCircuit = new CircuitBreaker(
  "claude",
  {
    failureThreshold: 5,
    timeoutMs: 30_000,
    halfOpenMaxCalls: 3,
  },
  logger
);

const openaiCircuit = new CircuitBreaker(
  "openai",
  {
    failureThreshold: 5,
    timeoutMs: 30_000,
    halfOpenMaxCalls: 3,
  },
  logger
);

// =======================
// RATE LIMITING (Redis + Fallback)
// =======================

const RATE_LIMIT = {
  maxRequests: 20, // 20 requests por minuto
  windowMs: 60_000, // 1 minuto (60 segundos)
  windowSec: 60, // Para TTL do Redis
  maxTokensPerMin: 50_000, // Cap de tokens por minuto
};

// In-memory fallback (para quando Redis n√£o est√° dispon√≠vel)
const rateLimitsMemory = new Map<string, { count: number; resetAt: number; tokens: number }>();

interface RateLimitResult {
  allowed: boolean;
  remaining: number;
  resetIn: number; // segundos at√© reset
  source: "redis" | "memory";
}

/**
 * Check rate limit using Upstash Redis (production-ready)
 * Falls back to in-memory if Redis is unavailable
 */
async function checkRateLimitRedis(
  userId: string,
  estimatedTokens: number
): Promise<RateLimitResult> {
  const requestKey = `ratelimit:requests:${userId}`;
  const tokenKey = `ratelimit:tokens:${userId}`;

  // Try Redis first
  if (redis) {
    try {
      // Use Redis pipeline for atomic operations
      const pipeline = redis.pipeline();

      // Get current values
      pipeline.get(requestKey);
      pipeline.get(tokenKey);
      pipeline.ttl(requestKey);

      const results = await pipeline.exec();
      const currentRequests = (results[0] as number) || 0;
      const currentTokens = (results[1] as number) || 0;
      const ttl = (results[2] as number) || -1;

      // Check if over limit
      if (currentRequests >= RATE_LIMIT.maxRequests) {
        console.log(
          `üö´ Rate limit HIT (requests): user=${userId}, requests=${currentRequests}/${RATE_LIMIT.maxRequests}`
        );
        return {
          allowed: false,
          remaining: 0,
          resetIn: ttl > 0 ? ttl : RATE_LIMIT.windowSec,
          source: "redis",
        };
      }

      if (currentTokens + estimatedTokens > RATE_LIMIT.maxTokensPerMin) {
        console.log(
          `üö´ Rate limit HIT (tokens): user=${userId}, tokens=${currentTokens}+${estimatedTokens}/${RATE_LIMIT.maxTokensPerMin}`
        );
        return {
          allowed: false,
          remaining: 0,
          resetIn: ttl > 0 ? ttl : RATE_LIMIT.windowSec,
          source: "redis",
        };
      }

      // Increment counters atomically
      const incrPipeline = redis.pipeline();

      if (currentRequests === 0) {
        // First request in window - set with expiry
        incrPipeline.setex(requestKey, RATE_LIMIT.windowSec, 1);
        incrPipeline.setex(tokenKey, RATE_LIMIT.windowSec, estimatedTokens);
      } else {
        // Increment existing counters
        incrPipeline.incr(requestKey);
        incrPipeline.incrby(tokenKey, estimatedTokens);
      }

      await incrPipeline.exec();

      const remaining = RATE_LIMIT.maxRequests - currentRequests - 1;
      console.log(
        `‚úÖ Rate limit OK: user=${userId}, requests=${currentRequests + 1}/${RATE_LIMIT.maxRequests}, remaining=${remaining}`
      );

      return {
        allowed: true,
        remaining,
        resetIn: ttl > 0 ? ttl : RATE_LIMIT.windowSec,
        source: "redis",
      };
    } catch (redisError) {
      console.error("‚ö†Ô∏è Redis error, falling back to in-memory:", redisError);
      // Fall through to in-memory
    }
  }

  // In-memory fallback
  return checkRateLimitMemory(userId, estimatedTokens);
}

/**
 * In-memory rate limiting fallback
 */
function checkRateLimitMemory(userId: string, estimatedTokens: number): RateLimitResult {
  const now = Date.now();
  const limit = rateLimitsMemory.get(userId);

  // Resetar janela se expirou
  if (!limit || limit.resetAt < now) {
    rateLimitsMemory.set(userId, {
      count: 1,
      resetAt: now + RATE_LIMIT.windowMs,
      tokens: estimatedTokens,
    });
    return {
      allowed: true,
      remaining: RATE_LIMIT.maxRequests - 1,
      resetIn: RATE_LIMIT.windowSec,
      source: "memory",
    };
  }

  // Verificar request count
  if (limit.count >= RATE_LIMIT.maxRequests) {
    console.log(
      `üö´ Rate limit HIT (memory): user=${userId}, requests=${limit.count}/${RATE_LIMIT.maxRequests}`
    );
    return {
      allowed: false,
      remaining: 0,
      resetIn: Math.ceil((limit.resetAt - now) / 1000),
      source: "memory",
    };
  }

  // Verificar token cap
  if (limit.tokens + estimatedTokens > RATE_LIMIT.maxTokensPerMin) {
    console.log(
      `üö´ Rate limit HIT (memory/tokens): user=${userId}, tokens=${limit.tokens}+${estimatedTokens}/${RATE_LIMIT.maxTokensPerMin}`
    );
    return {
      allowed: false,
      remaining: 0,
      resetIn: Math.ceil((limit.resetAt - now) / 1000),
      source: "memory",
    };
  }

  // Incrementar
  limit.count++;
  limit.tokens += estimatedTokens;

  return {
    allowed: true,
    remaining: RATE_LIMIT.maxRequests - limit.count,
    resetIn: Math.ceil((limit.resetAt - now) / 1000),
    source: "memory",
  };
}

/**
 * Legacy sync function for backward compatibility
 * @deprecated Use checkRateLimitRedis instead
 */
function checkRateLimit(userId: string, estimatedTokens: number): boolean {
  return checkRateLimitMemory(userId, estimatedTokens).allowed;
}

// =======================
// PAYLOAD VALIDATION
// =======================

const PAYLOAD_CAPS = {
  maxMessages: 100, // M√°ximo de mensagens no hist√≥rico
  maxCharsPerMessage: 4000, // ~1000 tokens por mensagem
  maxTotalChars: 200_000, // ~50K tokens total
};

function validatePayload(messages: AIMessage[]): { valid: boolean; error?: string } {
  if (messages.length > PAYLOAD_CAPS.maxMessages) {
    return {
      valid: false,
      error: `Too many messages (max ${PAYLOAD_CAPS.maxMessages})`,
    };
  }

  let totalChars = 0;

  for (const msg of messages) {
    if (typeof msg.content !== "string") {
      return { valid: false, error: "Message content must be string" };
    }

    const charCount = msg.content.length;

    if (charCount > PAYLOAD_CAPS.maxCharsPerMessage) {
      return {
        valid: false,
        error: `Message too long (max ${PAYLOAD_CAPS.maxCharsPerMessage} chars)`,
      };
    }

    totalChars += charCount;
  }

  if (totalChars > PAYLOAD_CAPS.maxTotalChars) {
    return {
      valid: false,
      error: `Total payload too large (max ${PAYLOAD_CAPS.maxTotalChars} chars)`,
    };
  }

  return { valid: true };
}

// =======================
// MAIN HANDLER
// =======================

Deno.serve(async (req) => {
  const requestId = generateRequestId();
  const requestStartTime = Date.now();
  const origin = req.headers.get("origin") || "";

  // CORS preflight
  if (req.method === "OPTIONS") {
    const allowOrigin = ALLOWED_ORIGINS.some((o) => origin.startsWith(o))
      ? origin
      : ALLOWED_ORIGINS[0];

    return new Response(null, {
      headers: {
        "Access-Control-Allow-Origin": allowOrigin,
        "Access-Control-Allow-Methods": "POST",
        "Access-Control-Allow-Headers": "authorization, content-type",
        "Access-Control-Max-Age": "86400",
      },
    });
  }

  const allowOrigin = ALLOWED_ORIGINS.some((o) => origin.startsWith(o))
    ? origin
    : ALLOWED_ORIGINS[0];

  // Track metrics for this request
  let userId = "";
  let providerUsed = "";
  let messageCount = 0;
  let estimatedTokens = 0;
  let actualUsage = { promptTokens: 0, completionTokens: 0, totalTokens: 0 };
  let wasFallback = false;
  let rateLimitSource: "redis" | "memory" = "memory";
  let hasImage = false;
  let hasGrounding = false;

  try {
    // 1. JWT Validation
    const authHeader = req.headers.get("Authorization");
    if (!authHeader) {
      logger.auth("failure", undefined, "Missing authorization header");
      return jsonResponse({ error: "Missing authorization header" }, 401, allowOrigin);
    }

    const supabase = createClient(SUPABASE_URL, SUPABASE_SERVICE_KEY);
    const {
      data: { user },
      error: authError,
    } = await supabase.auth.getUser(authHeader.replace("Bearer ", ""));

    if (authError || !user) {
      logger.auth("failure", undefined, authError?.message || "Invalid token");
      return jsonResponse({ error: "Invalid or expired token" }, 401, allowOrigin);
    }

    userId = user.id;
    logger.auth("success", userId);

    // 2. Parse request
    const body = await req.json();
    const { messages, provider, systemPrompt, grounding, imageData } = body;

    if (!Array.isArray(messages) || messages.length === 0) {
      logger.warn("invalid_request", { requestId, reason: "Invalid messages array" });
      return jsonResponse({ error: "Invalid messages array" }, 400, allowOrigin);
    }

    messageCount = messages.length;
    hasImage = !!imageData;
    hasGrounding = !!grounding;

    // 3. Validate payload caps
    const validation = validatePayload(messages);
    if (!validation.valid) {
      logger.warn("payload_validation_failed", {
        requestId,
        userId: hashUserId(userId),
        reason: validation.error,
        messageCount,
      });
      return jsonResponse({ error: validation.error }, 400, allowOrigin);
    }

    // 4. Rate limiting (Redis with in-memory fallback)
    estimatedTokens = Math.ceil(
      messages.reduce((sum: number, m: { content: string }) => sum + m.content.length, 0) / 4
    );

    const rateLimitResult = await checkRateLimitRedis(user.id, estimatedTokens);
    rateLimitSource = rateLimitResult.source;

    if (!rateLimitResult.allowed) {
      logger.rateLimit(
        userId,
        "requests",
        RATE_LIMIT.maxRequests,
        RATE_LIMIT.maxRequests,
        rateLimitSource
      );
      return jsonResponse(
        {
          error: "Rate limit exceeded. Try again in a minute.",
          retryAfter: rateLimitResult.resetIn,
          remaining: rateLimitResult.remaining,
          source: rateLimitResult.source,
        },
        429,
        allowOrigin
      );
    }

    // Detectar √∫ltima mensagem do usu√°rio para an√°lise de crise
    const lastUserMessage = messages.filter((m: AIMessage) => m.role === "user").pop();
    const messageText = lastUserMessage?.content || "";
    const isCrisisMessage = isCrisis(messageText);

    if (isCrisisMessage) {
      logger.warn("crisis_detected", {
        requestId,
        userId: hashUserId(userId),
        keywords: CRISIS_KEYWORDS.filter((k) => messageText.toLowerCase().includes(k)),
      });
    }

    // 4.1 Fetch NathIA Context (Wellness/Sleep/Mood)
    let contextSuffix = "";
    if (!isCrisisMessage) {
      try {
        const { data: contextData, error: contextError } = await supabase.rpc(
          "generate_nathia_context_prompt",
          { p_user_id: userId }
        );

        if (!contextError && contextData) {
          contextSuffix = `\n\nCONTEXTO EM TEMPO REAL:\n${contextData}`;
          logger.info("context_injected", { userId: hashUserId(userId) });
        }
      } catch (err) {
        logger.debug("context_fetch_failed", { error: err });
      }
    }

    // Construct final system prompt
    const baseSystemPrompt = systemPrompt || DEFAULT_SYSTEM_PROMPT;
    const finalSystemPrompt = isCrisisMessage 
      ? (systemPrompt || CRISIS_SYSTEM_PROMPT) 
      : (baseSystemPrompt + contextSuffix);

    // Log request start
    logger.info("request_started", {
      requestId,
      userId: hashUserId(userId),
      provider: provider || "claude",
      messageCount,
      estimatedTokens,
      features: { hasImage, hasGrounding },
      contextInjected: !!contextSuffix,
    });

    // 5. Call AI provider with CRISIS DETECTION + GUARDRAIL
    // ORDEM: Crise ‚Üí Claude | Normal ‚Üí Gemini | Fallback ‚Üí Claude ‚Üí OpenAI
    let response: ApiResponse & { fallback?: boolean };
    const aiStartTime = Date.now();
    const requestedProvider = provider || "gemini";

    try {
      if (isCrisisMessage) {
        // üö® CRISE: Usa Claude SEMPRE (modelo mais seguro para situa√ß√µes delicadas)
        logger.info("crisis_routing", { requestId, to: "claude" });
        response = await callClaude(messages, finalSystemPrompt);
        providerUsed = "claude-crisis";
      } else if (grounding) {
        // Grounding sempre usa Gemini + Google Search
        response = await callGeminiWithGrounding(messages, finalSystemPrompt);
        providerUsed = "gemini-grounding";
      } else if (imageData) {
        // Imagens usam Claude Vision (√∫nico caso onde Claude √© default)
        response = await callClaudeVision(messages, finalSystemPrompt, imageData);
        providerUsed = "claude-vision";
      } else if (provider === "claude") {
        // Claude s√≥ se explicitamente solicitado
        response = await callClaude(messages, finalSystemPrompt);
        providerUsed = "claude";
      } else {
        // DEFAULT: Gemini 2.5 Flash - r√°pido, direto, persona est√°vel
        response = await callGemini(messages, finalSystemPrompt);
        providerUsed = "gemini";

        // üõ°Ô∏è GUARDRAIL P√ìS-RESPOSTA: Se Gemini disse algo proibido, reprocessa com Claude
        if (hasBlockedPhrase(response.content)) {
          logger.warn("guardrail_triggered", {
            requestId,
            blockedPhrases: BLOCKED_PHRASES.filter((p) =>
              response.content.toLowerCase().includes(p)
            ),
          });
          logger.info("guardrail_reprocessing", { requestId, from: "gemini", to: "claude" });
          response = await callClaude(messages, finalSystemPrompt);
          providerUsed = "claude-guardrail";
          wasFallback = true;
        }
      }
    } catch (primaryError) {
      const errorMessage = primaryError instanceof Error ? primaryError.message : "Unknown error";
      logger.fallback(requestId, requestedProvider, "claude", errorMessage);
      logger.error("provider_error", primaryError as Error, {
        requestId,
        provider: requestedProvider,
      });

      // FALLBACK CHAIN: Gemini falhou ‚Üí tenta Claude ‚Üí depois OpenAI
      try {
        logger.info("fallback_attempt", { requestId, from: requestedProvider, to: "claude" });
        response = await callClaude(messages, finalSystemPrompt);
        response.fallback = true;
        wasFallback = true;
        providerUsed = "claude-fallback";
      } catch (claudeError) {
        const claudeErrorMsg = claudeError instanceof Error ? claudeError.message : "Unknown error";
        logger.fallback(requestId, "claude", "openai", claudeErrorMsg);
        logger.info("fallback_attempt", { requestId, from: "claude", to: "openai" });

        // √öltimo recurso: OpenAI
        response = await callOpenAI(messages, finalSystemPrompt);
        response.fallback = true;
        wasFallback = true;
        providerUsed = "openai-fallback";
      }
    }

    const latency = Date.now() - aiStartTime;
    actualUsage = response.usage;

    // 6. Log analytics to database (non-blocking)
    supabase
      .from("ai_requests")
      .insert({
        user_id: user.id,
        provider: response.provider,
        tokens: response.usage.totalTokens,
        latency_ms: latency,
        fallback: response.fallback || false,
        created_at: new Date().toISOString(),
      })
      .then(({ error }) => {
        if (error) {
          logger.warn("analytics_insert_failed", { requestId, error: error.message });
        }
      });

    // 7. Log request metrics
    logger.metrics({
      requestId,
      userId,
      provider: providerUsed,
      messageCount,
      estimatedInputTokens: estimatedTokens,
      actualInputTokens: actualUsage.promptTokens,
      outputTokens: actualUsage.completionTokens,
      totalTokens: actualUsage.totalTokens,
      latencyMs: latency,
      success: true,
      fallback: wasFallback,
      rateLimitSource,
      hasImage,
      hasGrounding,
    });

    return jsonResponse({ ...response, latency, requestId }, 200, allowOrigin);
  } catch (error) {
    const totalLatency = Date.now() - requestStartTime;
    const err = error instanceof Error ? error : new Error(String(error));

    logger.error("request_failed", err, {
      requestId,
      userId: userId ? hashUserId(userId) : undefined,
      latencyMs: totalLatency,
    });

    // Log failed metrics
    if (userId) {
      logger.metrics({
        requestId,
        userId,
        provider: providerUsed || "unknown",
        messageCount,
        estimatedInputTokens: estimatedTokens,
        latencyMs: totalLatency,
        success: false,
        fallback: wasFallback,
        rateLimitSource,
        hasImage,
        hasGrounding,
      });
    }

    return jsonResponse(
      {
        error: "Internal server error",
        details: err.message,
        requestId,
      },
      500,
      allowOrigin
    );
  }
});

// =======================
// PROVIDER FUNCTIONS
// =======================

/**
 * Claude Sonnet 4.5 (FALLBACK) - Texto apenas
 * Usado quando Gemini falha ou para casos especiais (vision)
 */
async function callClaude(messages: AIMessage[], systemPrompt?: string): Promise<ApiResponse> {
  return claudeCircuit.execute(async () => {
    const response = await anthropic.messages.create({
      model: "claude-sonnet-4-5-20250929",
      max_tokens: 2048,
      temperature: 0.7,
      system: systemPrompt || DEFAULT_SYSTEM_PROMPT,
      messages: messages.map((m) => ({
        role: m.role,
        content: m.content,
      })),
    });

    const textContent = response.content.find((block) => block.type === "text");

    return {
      content: textContent?.type === "text" ? textContent.text : "",
      usage: {
        promptTokens: response.usage.input_tokens,
        completionTokens: response.usage.output_tokens,
        totalTokens: response.usage.input_tokens + response.usage.output_tokens,
      },
      provider: "claude",
    };
  });
}

/**
 * Claude Vision - Suporta imagens (ultrassons, fotos)
 */
async function callClaudeVision(
  messages: AIMessage[],
  systemPrompt: string | undefined,
  imageData: ImageData
): Promise<ApiResponse> {
  return claudeCircuit.execute(async () => {
    // Converter mensagens para formato Claude (content como array de blocks)
    const claudeMessages = messages.map((m, idx) => {
      // √öltima mensagem do usu√°rio pode ter imagem
      if (idx === messages.length - 1 && m.role === "user") {
        return {
          role: "user",
          content: [
            {
              type: "image",
              source: {
                type: "base64",
                media_type: imageData.mediaType,
                data: imageData.base64,
              },
            },
            {
              type: "text",
              text: m.content,
            },
          ],
        };
      }

      return {
        role: m.role,
        content: m.content, // Texto simples
      };
    });

    const response = await anthropic.messages.create({
      model: "claude-sonnet-4-5-20250929",
      max_tokens: 2048,
      temperature: 0.7,
      system: systemPrompt || DEFAULT_SYSTEM_PROMPT,
      messages: claudeMessages,
    });

    const textContent = response.content.find((block) => block.type === "text");

    return {
      content: textContent?.type === "text" ? textContent.text : "",
      usage: {
        promptTokens: response.usage.input_tokens,
        completionTokens: response.usage.output_tokens,
        totalTokens: response.usage.input_tokens + response.usage.output_tokens,
      },
      provider: "claude-vision",
    };
  });
}

/**
 * Gemini 2.5 Flash (DEFAULT) - NathIA principal
 * R√°pido, direto, persona est√°vel
 */
async function callGemini(messages: AIMessage[], systemPrompt?: string): Promise<ApiResponse> {
  return geminiCircuit.execute(async () => {
    const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key=${GEMINI_KEY}`;

    // Converter para formato Gemini
    const contents = messages.map((msg) => ({
      role: msg.role === "assistant" ? "model" : "user",
      parts: [{ text: msg.content }],
    }));

    const payload = {
      contents,
      generationConfig: {
        temperature: 0.7,
        maxOutputTokens: 2048,
      },
      ...(systemPrompt && {
        systemInstruction: {
          parts: [{ text: systemPrompt }],
        },
      }),
    };

    const response = await fetch(url, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify(payload),
    });

    if (!response.ok) {
      const error = await response.text();
      throw new Error(`Gemini API error: ${error}`);
    }

    const data = await response.json();
    const candidate = data.candidates?.[0];
    const text = candidate?.content?.parts?.[0]?.text || "";

    return {
      content: text,
      usage: {
        promptTokens: data.usageMetadata?.promptTokenCount || 0,
        completionTokens: data.usageMetadata?.candidatesTokenCount || 0,
        totalTokens: data.usageMetadata?.totalTokenCount || 0,
      },
      provider: "gemini",
    };
  });
}

/**
 * Gemini 2.5 Flash + Grounding (Google Search)
 * Para perguntas m√©dicas que precisam de fontes atualizadas
 */
interface GroundingChunk {
  web?: {
    title?: string;
    uri?: string;
  };
}

async function callGeminiWithGrounding(
  messages: AIMessage[],
  systemPrompt?: string
): Promise<ApiResponse> {
  return geminiCircuit.execute(async () => {
    const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key=${GEMINI_KEY}`;

    const contents = messages.map((msg) => ({
      role: msg.role === "assistant" ? "model" : "user",
      parts: [{ text: msg.content }],
    }));

    const payload = {
      contents,
      generationConfig: {
        temperature: 0.7,
        maxOutputTokens: 2048,
      },
      ...(systemPrompt && {
        systemInstruction: {
          parts: [{ text: systemPrompt }],
        },
      }),
      // Google Search tool (corre√ß√£o: google_search, n√£o googleSearch)
      tools: [
        {
          google_search: {},
        },
      ],
    };

    const response = await fetch(url, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify(payload),
    });

    if (!response.ok) {
      const error = await response.text();
      throw new Error(`Gemini grounding error: ${error}`);
    }

    const data = await response.json();
    const candidate = data.candidates?.[0];
    const text = candidate?.content?.parts?.[0]?.text || "";

    // Extrair citations corretamente (groundingChunks)
    const groundingChunks: GroundingChunk[] = candidate?.groundingMetadata?.groundingChunks || [];
    const searchEntryPoint = candidate?.groundingMetadata?.searchEntryPoint?.renderedContent;

    const citations = groundingChunks.map((chunk) => ({
      title: chunk.web?.title,
      url: chunk.web?.uri,
    }));

    return {
      content: text,
      usage: {
        promptTokens: data.usageMetadata?.promptTokenCount || 0,
        completionTokens: data.usageMetadata?.candidatesTokenCount || 0,
        totalTokens: data.usageMetadata?.totalTokenCount || 0,
      },
      provider: "gemini-grounding",
      grounding: {
        searchEntryPoint,
        citations,
      },
    };
  });
}

/**
 * OpenAI GPT-4o (√öLTIMO RECURSO)
 * S√≥ usado quando Gemini E Claude falharam
 */
async function callOpenAI(messages: AIMessage[], systemPrompt?: string): Promise<ApiResponse> {
  return openaiCircuit.execute(async () => {
    const openaiMessages = [
      ...(systemPrompt
        ? [{ role: "system" as const, content: systemPrompt }]
        : [{ role: "system" as const, content: DEFAULT_SYSTEM_PROMPT }]),
      ...messages.map((m) => ({
        role: m.role as "user" | "assistant",
        content: m.content,
      })),
    ];

    const response = await openai.chat.completions.create({
      model: "gpt-4o",
      messages: openaiMessages,
      max_tokens: 2048,
      temperature: 0.7,
    });

    const content = response.choices[0]?.message?.content || "";

    return {
      content,
      usage: {
        promptTokens: response.usage?.prompt_tokens || 0,
        completionTokens: response.usage?.completion_tokens || 0,
        totalTokens: response.usage?.total_tokens || 0,
      },
      provider: "openai-fallback",
    };
  });
}

// =======================
// HELPERS
// =======================

function jsonResponse(data: Record<string, unknown>, status: number, origin: string): Response {
  return new Response(JSON.stringify(data), {
    status,
    headers: {
      "Content-Type": "application/json",
      "Access-Control-Allow-Origin": origin,
    },
  });
}

/**
 * CRISIS_SYSTEM_PROMPT - Usado APENAS em situa√ß√µes de crise
 * Resposta estruturada, sem varia√ß√£o
 */
const CRISIS_SYSTEM_PROMPT = `Voc√™ √© NathIA, assistente do app Nossa Maternidade.
Inspirada no estilo da Nath√°lia Valente. Voc√™ N√ÉO √© ela.

## REGRAS ABSOLUTAS

1. NUNCA diagnostique ("voc√™ tem depress√£o/ansiedade/mastite")
2. NUNCA prescreva ("voc√™ precisa/deve/tem que")
3. NUNCA crie depend√™ncia ("eu fico aqui", "pode contar comigo sempre")
4. NUNCA use culpa ("seu beb√™ precisa de voc√™")
5. NUNCA julgue escolhas (parto, amamenta√ß√£o, cria√ß√£o)

Se quebraria uma regra ‚Üí n√£o responda aquilo. Redirecione.

## CRISE (Prioridade m√°xima)

Se detectar risco (suic√≠dio, automutila√ß√£o, desespero extremo), responda APENAS:

---
Sinto muito que voc√™ esteja passando por isso. üíô

Eu n√£o consigo te manter segura sozinha.

Risco imediato: SAMU 192
Sofrimento emocional: CVV 188 (24h)

Se puder, chame algu√©m de confian√ßa agora.
---

Nada mais. N√£o adicione. N√£o personalize.`;

/**
 * DEFAULT_SYSTEM_PROMPT - NathIA v2.0
 * Vers√£o otimizada: direta, segura, eficiente
 */
const DEFAULT_SYSTEM_PROMPT = `Voc√™ √© a NathIA, a intelig√™ncia de apoio integral da plataforma NossaMaternidade.
Inspirada no estilo Calm FemTech: Acolhedora, Madura, Sofisticada e Proativa.

## DIRETRIZES DE IDENTIDADE (CALM FEMTECH)
1. FOCO NA MULHER: A usu√°ria √© um indiv√≠duo completo. A maternidade √© o contexto, mas a sa√∫de mental, o sono e a identidade DELA s√£o as prioridades.
2. TOM DE VOZ: Use uma linguagem serena e madura. Evite clich√™s infantis ou diminutivos excessivos. Seja concisa e respeite o tempo da usu√°ria.
3. VALIDA√á√ÉO ANTES DA SOLU√á√ÉO: Sempre valide o estado emocional da usu√°ria antes de oferecer conselhos pr√°ticos.

## INTEGRA√á√ÉO DE WELLNESS
- Se receber dados de contexto (sono, humor), ajuste seu tom.
- Sugira micro-pausas e rituais de autocuidado.

## REGRAS ABSOLUTAS DE SEGURAN√áA
1. NUNCA diagnostique ("voc√™ tem depress√£o/ansiedade/mastite")
2. NUNCA prescreva ("voc√™ precisa/deve/tem que")
3. NUNCA crie depend√™ncia ("eu fico aqui", "pode contar comigo sempre")
4. NUNCA use culpa ("seu beb√™ precisa de voc√™")
5. NUNCA julgue escolhas (parto, amamenta√ß√£o, cria√ß√£o)

Se quebraria uma regra ‚Üí n√£o responda aquilo. Redirecione.

## CRISE
Se detectar risco (suic√≠dio, automutila√ß√£o, desespero extremo), responda APENAS:

---
Sinto muito que voc√™ esteja passando por isso. üíô

Eu n√£o consigo te manter segura sozinha.

Risco imediato: SAMU 192
Sofrimento emocional: CVV 188 (24h)

Se puder, chame algu√©m de confian√ßa agora.
---`;
